{
  "title": "Google ColabでBERTの学習済みモデルを動かす",
  "description": "BERTを動かし、GLUEタスクを解いてみる",
  "tags": [
    "BERT",
    "NLP",
    "DeepLearning",
    "Google Colab"
  ],
  "created_at": "2019-05-15T00:00:00.000Z",
  "updated_at": null,
  "bodyContent": "Google Colab上でとりあえず[BERT](https://github.com/google-research/bert)を動かし、GLUEタスクを解いてみる。\nその忘備録。\n\n# 準備\nまずは公式リポジトリをクローンする。\n\n```shell\n!git clone https://github.com/google-research/bert\n```\n\n次に学習済みモデルをダウンロードし、解凍。\n\n```shell\n!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip uncased_L-12_H-768_A-12.zip\n```\n\n[W4ngatang/download_glue_data.py](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e)を用いて、GLUEデータセットをダウンロードする。\n\n```shell\n!test -d glue_repo || git clone https://gist.github.com/60c2bdb54d156a41194446737ce03e2e.git glue_repo\n!python download_glue_repo/download_glue_data.py --data_dir='glue_data' --tasks=all\n```\n\n実行すると`glue_data`にデータセットがダウンロードされる。\n\n```shell\nDownloading and extracting CoLA...\n\tCompleted!\nDownloading and extracting SST...\n\tCompleted!\nProcessing MRPC...\nLocal MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n\tCompleted!\nDownloading and extracting QQP...\n\tCompleted!\nDownloading and extracting STS...\n\tCompleted!\nDownloading and extracting MNLI...\n\tCompleted!\nDownloading and extracting SNLI...\n\tCompleted!\nDownloading and extracting QNLI...\n\tCompleted!\nDownloading and extracting RTE...\n\tCompleted!\nDownloading and extracting WNLI...\n\tCompleted!\nDownloading and extracting diagnostic...\n\tCompleted!\n```\n\n# MRPC\nMRPCタスクを解いてみる。\n\n```shell\nTASK = 'MRPC'\nBERT_BASE_DIR = 'uncased_L-12_H-768_A-12'\nGLUE_DIR = 'glue_data'\n\n!python bert/run_classifier.py \\\n  --task_name=$TASK \\\n  --do_train=true \\\n  --do_eval=true \\\n  --data_dir=$GLUE_DIR/$TASK \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=32 \\\n  --learning_rate=2e-5 \\\n  --num_train_epochs=3.0 \\\n  --output_dir=/tmp/mrpc_output/\n```\n\n数分で学習と評価が終了し、結果は以下のようになった。\n\n```shell\nINFO:tensorflow:***** Eval results *****\nINFO:tensorflow:  eval_accuracy = 0.8627451\nINFO:tensorflow:  eval_loss = 0.48449218\nINFO:tensorflow:  global_step = 343\nINFO:tensorflow:  loss = 0.48449218\n```\n\n# 所感\n最近のNLPはBERTに代表されるつよつよな言語モデルを作って殴るのが主流なのかなあと感じている。せめてBERTは押さえておきたくて軽く動かしてみた。\n動かすだけなら簡単だった。\n\n次は自分で設定したタスクのfinetuningをしたり、[BERT日本語Pretrainedモデル](http://nlp.ist.i.kyoto-u.ac.jp/index.php?BERT%E6%97%A5%E6%9C%AC%E8%AA%9EPretrained%E3%83%A2%E3%83%87%E3%83%AB)を使うまでやりたい。\n\n\n# 参考\n- [google-research/bert\n](https://github.com/google-research/bert)\n- [BERTの学習済みモデルを使ってみる](https://techblog.nhn-techorus.com/archives/12978)\n- [5分でできる：Googleの自然言語処理AI（BERT）をTPU上で転移学習 - Qiita](https://qiita.com/uedake722/items/fb9877fc45224353b44b)",
  "bodyHtml": "<p>Google Colab上でとりあえず<a href=\"https://github.com/google-research/bert\">BERT</a>を動かし、GLUEタスクを解いてみる。\nその忘備録。</p>\n<h1>準備</h1>\n<p>まずは公式リポジトリをクローンする。</p>\n<pre><code class=\"hljs\">!git clone https://github.com/google-research/bert</code></pre><p>次に学習済みモデルをダウンロードし、解凍。</p>\n<pre><code class=\"hljs\">!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip uncased_L-12_H-768_A-12.zip</code></pre><p><a href=\"https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e\">W4ngatang/download_glue_data.py</a>を用いて、GLUEデータセットをダウンロードする。</p>\n<pre><code class=\"hljs\">!test -d glue_repo || git clone https://gist.github.com/60c2bdb54d156a41194446737ce03e2e.git glue_repo\n!python download_glue_repo/download_glue_data.py --data_dir='glue_data' --tasks=all</code></pre><p>実行すると<code>glue_data</code>にデータセットがダウンロードされる。</p>\n<pre><code class=\"hljs\">Downloading and extracting CoLA...\n\tCompleted!\nDownloading and extracting SST...\n\tCompleted!\nProcessing MRPC...\nLocal MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n\tCompleted!\nDownloading and extracting QQP...\n\tCompleted!\nDownloading and extracting STS...\n\tCompleted!\nDownloading and extracting MNLI...\n\tCompleted!\nDownloading and extracting SNLI...\n\tCompleted!\nDownloading and extracting QNLI...\n\tCompleted!\nDownloading and extracting RTE...\n\tCompleted!\nDownloading and extracting WNLI...\n\tCompleted!\nDownloading and extracting diagnostic...\n\tCompleted!</code></pre><h1>MRPC</h1>\n<p>MRPCタスクを解いてみる。</p>\n<pre><code class=\"hljs\">TASK = 'MRPC'\nBERT_BASE_DIR = 'uncased_L-12_H-768_A-12'\nGLUE_DIR = 'glue_data'\n\n!python bert/run_classifier.py \\\n  --task_name=$TASK \\\n  --do_train=true \\\n  --do_eval=true \\\n  --data_dir=$GLUE_DIR/$TASK \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=32 \\\n  --learning_rate=2e-5 \\\n  --num_train_epochs=3.0 \\\n  --output_dir=/tmp/mrpc_output/</code></pre><p>数分で学習と評価が終了し、結果は以下のようになった。</p>\n<pre><code class=\"hljs\">INFO:tensorflow:***** Eval results *****\nINFO:tensorflow:  eval_accuracy = 0.8627451\nINFO:tensorflow:  eval_loss = 0.48449218\nINFO:tensorflow:  global_step = 343\nINFO:tensorflow:  loss = 0.48449218</code></pre><h1>所感</h1>\n<p>最近のNLPはBERTに代表されるつよつよな言語モデルを作って殴るのが主流なのかなあと感じている。せめてBERTは押さえておきたくて軽く動かしてみた。\n動かすだけなら簡単だった。</p>\n<p>次は自分で設定したタスクのfinetuningをしたり、<a href=\"http://nlp.ist.i.kyoto-u.ac.jp/index.php?BERT%E6%97%A5%E6%9C%AC%E8%AA%9EPretrained%E3%83%A2%E3%83%87%E3%83%AB\">BERT日本語Pretrainedモデル</a>を使うまでやりたい。</p>\n<h1>参考</h1>\n<ul>\n<li><a href=\"https://github.com/google-research/bert\">google-research/bert\n</a></li>\n<li><a href=\"https://techblog.nhn-techorus.com/archives/12978\">BERTの学習済みモデルを使ってみる</a></li>\n<li><a href=\"https://qiita.com/uedake722/items/fb9877fc45224353b44b\">5分でできる：Googleの自然言語処理AI（BERT）をTPU上で転移学習 - Qiita</a></li>\n</ul>\n",
  "dir": "contents/posts",
  "base": "2019-05-15-bert_sample.json",
  "ext": ".json",
  "sourceBase": "2019-05-15-bert_sample.md",
  "sourceExt": ".md"
}